"""
Export Daily Analytics to PostgreSQL

Reads analytics data from MinIO and exports to PostgreSQL for Grafana visualization
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from datetime import datetime
import sys

print("=" * 70)
print("üìä DAILY ANALYTICS EXPORT TO POSTGRESQL")
print("=" * 70)
print(f"üïê Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("")

# Create Spark Session
spark = (
    SparkSession.builder.appName("Daily Analytics Export")
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000")
    .config("spark.hadoop.fs.s3a.access.key", "admin")
    .config("spark.hadoop.fs.s3a.secret.key", "password123")
    .config("spark.hadoop.fs.s3a.path.style.access", "true")
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
    .config(
        "spark.hadoop.fs.s3a.aws.credentials.provider",
        "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider",
    )
    .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")
    .getOrCreate()
)

print("‚úÖ Spark Session created")

# PostgreSQL connection
postgres_url = "jdbc:postgresql://postgres-analytics:5432/analytics_db"
postgres_properties = {
    "user": "analytics",
    "password": "analytics123",
    "driver": "org.postgresql.Driver",
}

print("‚úÖ PostgreSQL connection configured")
print("")

# ==========================================
# 1. EXPORT TOP PRODUCTS
# ==========================================
print("üì¶ Reading top products from MinIO...")
try:
    top_products_path = "s3a://hudi-warehouse/top_products_daily"
    top_products = spark.read.format("parquet").load(top_products_path)

    print(f"‚úÖ Read {top_products.count()} product records")
    print("Sample data:")
    top_products.show(5, truncate=False)

    print("üíæ Exporting to PostgreSQL...")
    top_products.write.jdbc(
        url=postgres_url,
        table="daily_top_products",
        mode="overwrite",
        properties=postgres_properties,
    )
    print("‚úÖ Top products exported to PostgreSQL")

except Exception as e:
    print(f"‚ö†Ô∏è Warning: Could not export top products - {e}")

print("")

# ==========================================
# 2. EXPORT TOP CUSTOMERS
# ==========================================
print("üë• Reading top customers from MinIO...")
try:
    top_customers_path = "s3a://hudi-warehouse/top_customers_daily"
    top_customers = spark.read.format("parquet").load(top_customers_path)

    print(f"‚úÖ Read {top_customers.count()} customer records")
    print("Sample data:")
    top_customers.show(5, truncate=False)

    print("üíæ Exporting to PostgreSQL...")
    top_customers.write.jdbc(
        url=postgres_url,
        table="daily_top_customers",
        mode="overwrite",
        properties=postgres_properties,
    )
    print("‚úÖ Top customers exported to PostgreSQL")

except Exception as e:
    print(f"‚ö†Ô∏è Warning: Could not export top customers - {e}")

print("")

# ==========================================
# 3. EXPORT STATUS DISTRIBUTION
# ==========================================
print("üìä Reading status distribution from MinIO...")
try:
    status_path = "s3a://hudi-warehouse/status_distribution_daily"
    status_dist = spark.read.format("parquet").load(status_path)

    print(f"‚úÖ Read {status_dist.count()} status records")
    print("Sample data:")
    status_dist.show(truncate=False)

    print("üíæ Exporting to PostgreSQL...")
    status_dist.write.jdbc(
        url=postgres_url,
        table="daily_status_distribution",
        mode="overwrite",
        properties=postgres_properties,
    )
    print("‚úÖ Status distribution exported to PostgreSQL")

except Exception as e:
    print(f"‚ö†Ô∏è Warning: Could not export status distribution - {e}")

print("")

# ==========================================
# 4. EXPORT HOURLY TRENDS
# ==========================================
print("‚è∞ Reading hourly trends from MinIO...")
try:
    hourly_path = "s3a://hudi-warehouse/hourly_trends_daily"
    hourly_trends = spark.read.format("parquet").load(hourly_path)

    print(f"‚úÖ Read {hourly_trends.count()} hourly records")
    print("Sample data:")
    hourly_trends.show(10, truncate=False)

    print("üíæ Exporting to PostgreSQL...")
    hourly_trends.write.jdbc(
        url=postgres_url,
        table="daily_hourly_trends",
        mode="overwrite",
        properties=postgres_properties,
    )
    print("‚úÖ Hourly trends exported to PostgreSQL")

except Exception as e:
    print(f"‚ö†Ô∏è Warning: Could not export hourly trends - {e}")

print("")

# ==========================================
# 5. EXPORT OVERALL METRICS (from Hudi)
# ==========================================
print("üìà Reading overall metrics from MinIO...")
try:
    analytics_path = "s3a://hudi-warehouse/daily_analytics"
    overall_metrics = spark.read.format("hudi").load(analytics_path)

    print(f"‚úÖ Read {overall_metrics.count()} metric records")
    print("Sample data:")
    overall_metrics.select(
        "processing_date",
        "total_orders",
        "total_revenue",
        "avg_order_value",
        "unique_customers",
        "unique_products",
    ).show(truncate=False)

    print("üíæ Exporting to PostgreSQL...")
    overall_metrics.select(
        col("processing_date"),
        col("total_orders"),
        col("total_revenue"),
        col("avg_order_value"),
        col("min_order"),
        col("max_order"),
        col("unique_customers"),
        col("unique_products"),
    ).write.jdbc(
        url=postgres_url,
        table="daily_overall_metrics",
        mode="overwrite",
        properties=postgres_properties,
    )
    print("‚úÖ Overall metrics exported to PostgreSQL")

except Exception as e:
    print(f"‚ö†Ô∏è Warning: Could not export overall metrics - {e}")

print("")

# ==========================================
# SUMMARY
# ==========================================
print("=" * 70)
print("üéâ DAILY ANALYTICS EXPORT COMPLETED!")
print("=" * 70)
print("")
print("üìä Exported Tables to PostgreSQL:")
print("   ‚úÖ daily_top_products")
print("   ‚úÖ daily_top_customers")
print("   ‚úÖ daily_status_distribution")
print("   ‚úÖ daily_hourly_trends")
print("   ‚úÖ daily_overall_metrics")
print("")
print("üåê Access Points:")
print("   ‚Ä¢ PostgreSQL: localhost:5433")
print("   ‚Ä¢ Grafana:    http://localhost:3000")
print("")
print("üìù Next Steps:")
print("   1. Verify tables in PostgreSQL")
print("   2. Create Grafana dashboard")
print("   3. Add to Airflow DAG for automation")
print("")
print("=" * 70)

spark.stop()
